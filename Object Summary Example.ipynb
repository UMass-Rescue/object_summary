{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:56:56.214772Z",
     "start_time": "2020-02-15T17:56:55.081874Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasannals/anaconda3/envs/tf_obj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prasannals/anaconda3/envs/tf_obj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prasannals/anaconda3/envs/tf_obj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prasannals/anaconda3/envs/tf_obj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prasannals/anaconda3/envs/tf_obj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prasannals/anaconda3/envs/tf_obj/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from object_summary import clf_folders_to_df, objects_in_categories, dump_to_json_file, read_json_from_file\n",
    "from tf_object_detection_util.inference_api import TFInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:56:56.218922Z",
     "start_time": "2020-02-15T17:56:56.216017Z"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('data/messy_vs_clean/sample/')\n",
    "out_path = Path('out/messy_or_clean/sample/')\n",
    "out_path.mkdir(exist_ok=True)\n",
    "\n",
    "def ls(path): return [p for p in path.glob('*')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"path\" variable is the directory containing a list of folders where each folder contains all images of a single category and the name of the folder is the label for the category.\n",
    "\n",
    "\"out_path\" is the path where the output visualizations (if visualize=True in \"objects_in_categories\" method) are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:56:56.234637Z",
     "start_time": "2020-02-15T17:56:56.220131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/messy_vs_clean/sample/messy'),\n",
       " PosixPath('data/messy_vs_clean/sample/clean')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, we're using a small subset from Messy vs Clean dataset from Kaggle\n",
    "# https://www.kaggle.com/cdawn1/messy-vs-clean-room\n",
    "ls(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass the path to the \".pb\" file (this is the inference graph. the model we use to perform object detection) and the \".pbtxt\" file (this provides the mapping from the categorical output of the model (say 0, 1, 2) to the labels of the corresponding objects (say \"cat\", \"dog\", \"horse\")) into the TFInference class. We then need to pass the created TFInference object into the \"objects_in_categories\" to perform the summarization.\n",
    "\n",
    "We need to download the pretrained object detection model from tensorflow. Use the below link to download a Faster RCNN model trained on [Open Images](https://opensource.google/projects/open-images-dataset) - \n",
    "\n",
    "pb file contained in -  http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_v4_2018_12_12.tar.gz\n",
    "\n",
    "pbtxt file - https://github.com/tensorflow/models/blob/master/research/object_detection/data/oid_v4_label_map.pbtxt\n",
    "\n",
    "To select other object detection models, go to the following link - \n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:56:56.299681Z",
     "start_time": "2020-02-15T17:56:56.294878Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_faster_rcnn_open_images():\n",
    "    MODEL_NAME = Path('/home/prasannals/models/research/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_v4_2018_12_12')\n",
    "    PATH_TO_FROZEN_GRAPH = MODEL_NAME / 'frozen_inference_graph.pb'\n",
    "    PATH_TO_LABELS = MODEL_NAME / 'oid_v4_label_map.pbtxt'\n",
    "\n",
    "    inf = TFInference(PATH_TO_FROZEN_GRAPH, PATH_TO_LABELS)\n",
    "    return inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:57:04.408128Z",
     "start_time": "2020-02-15T17:56:56.574458Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf_folders_to_df converts the input files into the required format in a DataFrame.\n",
    "# the output of the \"clf_folders_to_df\" will be the input to \"objects_in_categories\" function\n",
    "path_df = clf_folders_to_df(path)\n",
    "inf = load_faster_rcnn_open_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:57:04.418852Z",
     "start_time": "2020-02-15T17:57:04.409507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                                path category\n",
       " 0  /home/prasannals/object_detection/data/messy_v...    messy\n",
       " 1  /home/prasannals/object_detection/data/messy_v...    messy,\n",
       "                                                 path category\n",
       " 2  /home/prasannals/object_detection/data/messy_v...    clean\n",
       " 3  /home/prasannals/object_detection/data/messy_v...    clean]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OPTIONAL - split your \"path_df\" into several equal DataFrames \n",
    "### (useful if you intend to parallely run the code on different machines/GPUs)\n",
    "from object_summary.util import split_df\n",
    "\n",
    "split_df(path_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:57:04.429236Z",
     "start_time": "2020-02-15T17:57:04.420264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/prasannals/object_detection/data/messy_v...</td>\n",
       "      <td>messy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/prasannals/object_detection/data/messy_v...</td>\n",
       "      <td>messy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/prasannals/object_detection/data/messy_v...</td>\n",
       "      <td>clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/prasannals/object_detection/data/messy_v...</td>\n",
       "      <td>clean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path category\n",
       "0  /home/prasannals/object_detection/data/messy_v...    messy\n",
       "1  /home/prasannals/object_detection/data/messy_v...    messy\n",
       "2  /home/prasannals/object_detection/data/messy_v...    clean\n",
       "3  /home/prasannals/object_detection/data/messy_v...    clean"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:58:33.404276Z",
     "start_time": "2020-02-15T17:57:04.430857Z"
    }
   },
   "outputs": [],
   "source": [
    "res = objects_in_categories(path_df, inf, out_path, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:58:33.412169Z",
     "start_time": "2020-02-15T17:58:33.405342Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'num_detections': 3,\n",
       "  'detection_boxes': array([[0.4943506 , 0.02411438, 0.97711736, 0.98587483],\n",
       "         [0.55433404, 0.5395526 , 0.95111483, 1.        ],\n",
       "         [0.        , 0.        , 0.97139925, 1.        ]], dtype=float32),\n",
       "  'detection_scores': array([0.5345746 , 0.40630788, 0.3590906 ], dtype=float32),\n",
       "  'detection_classes': array([281, 281, 241]),\n",
       "  'detection_classes_translated': ['Table', 'Table', 'House'],\n",
       "  'category': 'messy'},\n",
       " {'num_detections': 7,\n",
       "  'detection_boxes': array([[0.7609224 , 0.31506044, 0.8543164 , 0.51453626],\n",
       "         [0.7031163 , 0.7674436 , 0.9546674 , 0.97680485],\n",
       "         [0.39719838, 0.39140877, 0.78039163, 0.87248987],\n",
       "         [0.6339927 , 0.4408361 , 0.718982  , 0.5389727 ],\n",
       "         [0.3982816 , 0.3679562 , 0.78282356, 0.8985413 ],\n",
       "         [0.18970141, 0.41172495, 0.6037266 , 0.9957504 ],\n",
       "         [0.22396113, 0.00430926, 0.7286406 , 0.460847  ]], dtype=float32),\n",
       "  'detection_scores': array([0.7762132 , 0.63690084, 0.6345127 , 0.6321289 , 0.4654676 ,\n",
       "         0.43773025, 0.36643672], dtype=float32),\n",
       "  'detection_classes': array([ 11, 136, 378,  11, 281, 141, 154]),\n",
       "  'detection_classes_translated': ['Toy',\n",
       "   'Box',\n",
       "   'Coffee table',\n",
       "   'Toy',\n",
       "   'Table',\n",
       "   'Studio couch',\n",
       "   'Couch'],\n",
       "  'category': 'messy'},\n",
       " {'num_detections': 6,\n",
       "  'detection_boxes': array([[0.34482846, 0.02153671, 0.70484424, 0.9774927 ],\n",
       "         [0.37383267, 0.34502256, 0.49231553, 0.44351768],\n",
       "         [0.3470022 , 0.00582942, 0.7138032 , 0.61620575],\n",
       "         [0.09474622, 0.6073878 , 0.14807892, 0.65687   ],\n",
       "         [0.        , 0.        , 0.8563262 , 1.        ],\n",
       "         [0.3502259 , 0.02137604, 0.7215391 , 1.        ]], dtype=float32),\n",
       "  'detection_scores': array([0.50714415, 0.35876566, 0.3512842 , 0.32265767, 0.31847787,\n",
       "         0.30690277], dtype=float32),\n",
       "  'detection_classes': array([141, 220, 141, 433, 241, 246]),\n",
       "  'detection_classes_translated': ['Studio couch',\n",
       "   'Pillow',\n",
       "   'Studio couch',\n",
       "   'Clothing',\n",
       "   'House',\n",
       "   'Sofa bed'],\n",
       "  'category': 'clean'},\n",
       " {'num_detections': 15,\n",
       "  'detection_boxes': array([[0.4980016 , 0.04032005, 0.9762016 , 0.5954145 ],\n",
       "         [0.45708156, 0.15683562, 0.9743931 , 0.9703107 ],\n",
       "         [0.501738  , 0.        , 0.78900695, 0.5928752 ],\n",
       "         [0.52882737, 0.02020994, 0.7246595 , 0.54656374],\n",
       "         [0.56253165, 0.77168715, 0.9999395 , 1.        ],\n",
       "         [0.01966334, 0.        , 0.9408871 , 1.        ],\n",
       "         [0.2945071 , 0.15749401, 0.3947959 , 0.34123653],\n",
       "         [0.52820337, 0.5342359 , 0.9753131 , 0.99083436],\n",
       "         [0.08102766, 0.06742434, 0.40569633, 0.75266236],\n",
       "         [0.14585944, 0.78573424, 0.47268143, 0.9852465 ],\n",
       "         [0.6017494 , 0.62919   , 0.864477  , 0.7912919 ],\n",
       "         [0.08620381, 0.03240013, 0.43307567, 0.7576805 ],\n",
       "         [0.28807637, 0.15305158, 0.40085354, 0.35000244],\n",
       "         [0.37912786, 0.00801661, 0.9852704 , 0.72219115],\n",
       "         [0.4888789 , 0.52991986, 0.9679052 , 1.        ]], dtype=float32),\n",
       "  'detection_scores': array([0.78175527, 0.685339  , 0.6548929 , 0.63346803, 0.6330423 ,\n",
       "         0.62773526, 0.5592632 , 0.52605456, 0.50502574, 0.41089413,\n",
       "         0.39746213, 0.3763726 , 0.36292493, 0.3271734 , 0.31697792],\n",
       "        dtype=float32),\n",
       "  'detection_classes': array([442, 442, 442, 442, 107, 241, 151, 107, 107, 485, 107, 345, 523,\n",
       "         107, 442]),\n",
       "  'detection_classes_translated': ['Countertop',\n",
       "   'Countertop',\n",
       "   'Countertop',\n",
       "   'Countertop',\n",
       "   'Cabinetry',\n",
       "   'House',\n",
       "   'Oven',\n",
       "   'Cabinetry',\n",
       "   'Cabinetry',\n",
       "   'Window',\n",
       "   'Cabinetry',\n",
       "   'Cupboard',\n",
       "   'Microwave oven',\n",
       "   'Cabinetry',\n",
       "   'Countertop'],\n",
       "  'category': 'clean'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T17:58:33.417209Z",
     "start_time": "2020-02-15T17:58:33.413310Z"
    }
   },
   "outputs": [],
   "source": [
    "dump_to_json_file(res, Path('out/messy_or_clean_res_sample.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
